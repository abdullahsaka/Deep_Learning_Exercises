{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MyAssignment-3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1_wwXvBkZem"
      },
      "source": [
        "# Import required packages\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipa2uL7TThko"
      },
      "source": [
        "## **QUESTION 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk4f8_34NWHC"
      },
      "source": [
        "**PART A: Build a Markov (n-gram) language model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCZtrMCTOc8k"
      },
      "source": [
        "We built a markov model (n gram=trigram) model. Torch package was used to extract the IMDB dataset and the function generates trigrams is passed to the preprocessing argument in the data. 'Field' function helps splitting each review into trigrams and 'freqs' function is used to get the count of each trigram. This data can be used to calculate probabilties and predictions are made my sampling these probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DdlxKfANa8o"
      },
      "source": [
        "# split each review into tokens with 3-gram\n",
        "def create_trigram(x):\n",
        "    res = []\n",
        "    n_grams = list(zip(*[x[i:] for i in range(3)]))\n",
        "    for n_gram in n_grams:\n",
        "        res.append(' '.join(n_gram))\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOaZQaBYNlet",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "0fa06b42-05d5-451d-ac96-cf0e1be129b3"
      },
      "source": [
        "SEED = 7\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Load IMDB datasets\n",
        "TEXT = data.Field(tokenize='spacy',preprocessing=create_trigram) # tokenize reviews based on 3-gram model\n",
        "LABEL = data.LabelField(dtype=torch.float)\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT,LABEL)\n",
        "print(vars(train_data.examples[0]))\n",
        "\n",
        "# Split data into train and test data at the ratio of 80:20\n",
        "train_data, valid_data = train_data.split(random_state=random.seed(SEED),split_ratio=0.8)\n",
        "TEXT.build_vocab(train_data) # create a dictionary from train data\n",
        "dictionary = dict(TEXT.vocab.freqs) # calculate frequency of trigrams\n",
        "len(dictionary) # of components in dictionary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\raclImdb_v1.tar.gz:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:09<00:00, 9.31MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'text': ['The year 2005', 'year 2005 saw', '2005 saw no', 'saw no fewer', 'no fewer than', 'fewer than 3', 'than 3 filmed', '3 filmed productions', 'filmed productions of', 'productions of H.', 'of H. G.', 'H. G. Wells', \"G. Wells '\", \"Wells ' great\", \"' great novel\", 'great novel ,', 'novel , \"', ', \" War', '\" War of', 'War of the', 'of the Worlds', 'the Worlds \"', 'Worlds \" .', '\" . This', '. This is', 'This is perhaps', 'is perhaps the', 'perhaps the least', 'the least well', 'least well -', 'well - known', '- known and', 'known and very', 'and very probably', 'very probably the', 'probably the best', 'the best of', 'best of them', 'of them .', 'them . No', '. No other', 'No other version', 'other version of', 'version of WotW', 'of WotW has', 'WotW has ever', 'has ever attempted', 'ever attempted not', 'attempted not only', 'not only to', 'only to present', 'to present the', 'present the story', 'the story very', 'story very much', 'very much as', 'much as Wells', 'as Wells wrote', 'Wells wrote it', 'wrote it ,', 'it , but', ', but also', 'but also to', 'also to create', 'to create the', 'create the atmosphere', 'the atmosphere of', 'atmosphere of the', 'of the time', 'the time in', 'time in which', 'in which it', 'which it was', 'it was supposed', 'was supposed to', 'supposed to take', 'to take place', 'take place :', 'place : the', ': the last', 'the last year', 'last year of', 'year of the', 'of the 19th', 'the 19th Century', '19th Century ,', 'Century , 1900', ', 1900 \\x85 ', '1900 \\x85  using', '\\x85  using Wells', \"using Wells '\", \"Wells ' original\", \"' original setting\", 'original setting ,', 'setting , in', ', in and', 'in and near', 'and near Woking', 'near Woking ,', 'Woking , England.<br', ', England.<br /><br', 'England.<br /><br />IMDb', '/><br />IMDb seems', '/>IMDb seems unfriendly', 'seems unfriendly to', 'unfriendly to what', 'to what they', 'what they regard', 'they regard as', 'regard as \"', 'as \" spoilers', '\" spoilers \"', 'spoilers \" .', '\" . That', '. That might', 'That might apply', 'might apply with', 'apply with some', 'with some films', 'some films ,', 'films , where', ', where the', 'where the ending', 'the ending might', 'ending might actually', 'might actually be', 'actually be a', 'be a surprise', 'a surprise ,', 'surprise , but', ', but with', 'but with regard', 'with regard to', 'regard to one', 'to one of', 'one of the', 'of the most', 'the most famous', 'most famous novels', 'famous novels in', 'novels in the', 'in the world', 'the world ,', 'world , it', ', it seems', 'it seems positively', 'seems positively silly', 'positively silly .', 'silly . I', '. I have', 'I have no', 'have no sympathy', 'no sympathy for', 'sympathy for people', 'for people who', 'people who have', 'who have neglected', 'have neglected to', 'neglected to read', 'to read one', 'read one of', 'one of the', 'of the seminal', 'the seminal works', 'seminal works in', 'works in English', 'in English literature', 'English literature ,', 'literature , so', ', so let', \"so let 's\", \"let 's get\", \"'s get right\", 'get right to', 'right to the', 'to the chase', 'the chase .', 'chase . The', '. The aliens', 'The aliens are', 'aliens are destroyed', 'are destroyed through', 'destroyed through catching', 'through catching an', 'catching an Earth', 'an Earth disease', 'Earth disease ,', 'disease , against', ', against which', 'against which they', 'which they have', 'they have no', 'have no immunity', 'no immunity .', 'immunity . If', '. If that', \"If that 's\", \"that 's a\", \"'s a spoiler\", 'a spoiler ,', 'spoiler , so', ', so be', 'so be it', 'be it ;', 'it ; after', '; after a', 'after a book', 'a book and', 'book and 3', 'and 3 other', '3 other films', 'other films (', 'films ( including', '( including the', 'including the 1953', 'the 1953 classic', '1953 classic )', 'classic ) ,', ') , you', ', you ought', 'you ought to', 'ought to know', 'to know how', 'know how this', 'how this ends.<br', 'this ends.<br /><br', 'ends.<br /><br />This', '/><br />This film', '/>This film ,', 'film , which', ', which follows', 'which follows Wells', \"follows Wells '\", \"Wells ' plot\", \"' plot in\", 'plot in the', 'in the main', 'the main ,', 'main , is', ', is also', 'is also very', 'also very cleverly', 'very cleverly presented', 'cleverly presented \\x96', 'presented \\x96 in', '\\x96 in a', 'in a way', 'a way that', 'way that might', 'that might put', 'might put many', 'put many viewers', 'many viewers off', 'viewers off due', 'off due to', 'due to their', 'to their ignorance', 'their ignorance of', 'ignorance of late', 'of late 19th', 'late 19th /', '19th / early', '/ early 20th', 'early 20th Century', '20th Century photography', 'Century photography .', 'photography . Although', '. Although filmed', 'Although filmed in', 'filmed in a', 'in a widescreen', 'a widescreen aspect', 'widescreen aspect ,', 'aspect , the', ', the film', 'the film goes', 'film goes to', 'goes to some', 'to some lengths', 'some lengths to', 'lengths to give', 'to give an', 'give an impression', 'an impression of', 'impression of contemporaneity', 'of contemporaneity .', 'contemporaneity . The', '. The general', 'The general coloration', 'general coloration of', 'coloration of skin', 'of skin and', 'skin and clothes', 'and clothes display', 'clothes display a', 'display a sepia', 'a sepia tint', 'sepia tint often', 'tint often found', 'often found in', 'found in old', 'in old photographs', 'old photographs (', 'photographs ( rather', '( rather than', 'rather than black', 'than black )', 'black ) .', ') . Colors', '. Colors are', 'Colors are often', 'are often reminiscent', 'often reminiscent of', 'reminiscent of hand', 'of hand -', 'hand - tinting', '- tinting .', 'tinting . At', '. At other', 'At other times', 'other times ,', 'times , colors', ', colors are', 'colors are washed', 'are washed out', 'washed out .', 'out . These', '. These variations', 'These variations are', 'variations are typical', 'are typical of', 'typical of early', 'of early films', 'early films ,', 'films , which', ', which did', \"which did n't\", \"did n't use\", \"n't use standardized\", 'use standardized celluloid', 'standardized celluloid stock', 'celluloid stock and', 'stock and therefore', 'and therefore presented', 'therefore presented a', 'presented a good', 'a good many', 'good many changes', 'many changes in', 'changes in print', 'in print quality', 'print quality ,', 'quality , even', ', even going', 'even going from', 'going from black', 'from black /', 'black / white', '/ white to', 'white to sepia', 'to sepia /', 'sepia / white', '/ white to', 'white to blue', 'to blue /', 'blue / white', '/ white to', 'white to reddish', 'to reddish /', 'reddish / white', '/ white and', 'white and so', 'and so on', 'so on \\x96', 'on \\x96 as', '\\x96 as you', \"as you 'll\", \"you 'll see\", \"'ll see on\", 'see on occasion', 'on occasion here', 'occasion here .', 'here . The', '. The special', 'The special effects', 'special effects are', 'effects are deliberately', 'are deliberately retrograde', 'deliberately retrograde ,', 'retrograde , of', ', of a', 'of a sort', 'a sort seen', 'sort seen even', 'seen even as', 'even as late', 'as late as', 'late as the', 'as the 1920s', 'the 1920s \\x96', '1920s \\x96 and', '\\x96 and yet', 'and yet the', 'yet the Martians', 'the Martians and', 'Martians and their', 'and their machines', 'their machines are', 'machines are very', 'are very much', 'very much as', 'much as Wells', 'as Wells described', 'Wells described them', 'described them and', 'them and have', 'and have a', 'have a more', 'a more nearly', 'more nearly realistic', 'nearly realistic \"', 'realistic \" feel', '\" feel \"', 'feel \" .', '\" . Some', '. Some of', 'Some of effects', 'of effects are', 'effects are really', 'are really awkward', 'really awkward \\x96', 'awkward \\x96 such', '\\x96 such as', 'such as the', 'as the destruction', 'the destruction of', 'destruction of Big', 'of Big Ben', 'Big Ben .', 'Ben . The', '. The acting', 'The acting is', 'acting is often', 'is often more', 'often more in', 'more in the', 'in the style', 'the style of', 'style of that', 'of that period', 'that period than', 'period than ours', 'than ours .', 'ours . Some', '. Some aspects', 'Some aspects of', 'aspects of Victorian', 'of Victorian dress', 'Victorian dress may', 'dress may appear', 'may appear odd', 'appear odd ,', 'odd , particularly', ', particularly the', 'particularly the use', 'the use of', 'use of pomade', 'of pomade or', 'pomade or brilliantine', 'or brilliantine on', 'brilliantine on head', 'on head and', 'head and facial', 'and facial hair.<br', 'facial hair.<br /><br', 'hair.<br /><br />This', '/><br />This film', '/>This film is', 'film is the', 'is the only', 'the only one', 'only one that', 'one that follows', 'that follows with', 'follows with some', 'with some closeness', 'some closeness Wells', \"closeness Wells '\", \"Wells ' original\", \"' original narrative\", 'original narrative \\x96', 'narrative \\x96 as', '\\x96 as has', 'as has been', 'has been noted', 'been noted .', 'noted . Viewers', '. Viewers may', 'Viewers may find', 'may find it', 'find it informative', 'it informative to', 'informative to note', 'to note plot', 'note plot details', 'plot details that', 'details that appear', 'that appear here', 'appear here that', 'here that are', 'that are occasionally', 'are occasionally retained', 'occasionally retained in', 'retained in other', 'in other versions', 'other versions of', 'versions of the', 'of the story', 'the story .', 'story . Wells', \". Wells '\", \"Wells ' description\", \"' description of\", 'description of the', 'of the Martians', 'the Martians \\x96', 'Martians \\x96 a', '\\x96 a giant', 'a giant head', 'giant head mounted', 'head mounted on', 'mounted on numerous', 'on numerous tentacles', 'numerous tentacles \\x96', 'tentacles \\x96 is', '\\x96 is effectively', 'is effectively portrayed', 'effectively portrayed .', 'portrayed . When', '. When the', 'When the Martian', 'the Martian machines', 'Martian machines appear', 'machines appear ,', 'appear , about', ', about an', 'about an hour', 'an hour into', 'hour into the', 'into the film', 'the film ,', 'film , they', ', they too', 'they too give', 'too give a', 'give a good', 'a good impression', 'good impression of', 'impression of how', 'of how Wells', 'how Wells described', 'Wells described them', 'described them .', 'them . Both', '. Both Wells', 'Both Wells and', 'Wells and this', 'and this film', 'this film do', 'film do an', 'do an excellent', 'an excellent job', 'excellent job of', 'job of portraying', 'of portraying the', 'portraying the progress', 'the progress of', 'progress of the', 'of the Martians', 'the Martians from', 'Martians from the', 'from the limited', 'the limited perspective', 'limited perspective (', 'perspective ( primarily', '( primarily )', 'primarily ) of', ') of rural', 'of rural England', 'rural England \\x96', 'England \\x96 plus', '\\x96 plus a', 'plus a few', 'a few scenes', 'few scenes in', 'scenes in London', 'in London (', 'London ( involving', '( involving the', 'involving the Narrator', \"the Narrator 's\", \"Narrator 's brother\", \"'s brother )\", 'brother ) .', ') . The', '. The director', 'The director is', 'director is unable', 'is unable to', 'unable to resist', 'to resist showing', 'resist showing the', 'showing the destruction', 'the destruction of', 'destruction of a', 'of a major', 'a major landmark', 'major landmark (', 'landmark ( Big', '( Big Ben', 'Big Ben )', 'Ben ) ,', ') , but', ', but at', 'but at least', 'at least does', \"least does n't\", \"does n't dwell\", \"n't dwell unduly\", 'dwell unduly on', 'unduly on the', 'on the devastation', 'the devastation of', 'devastation of London.<br', 'of London.<br /><br', 'London.<br /><br />The', '/><br />The victory', '/>The victory of', 'victory of the', 'of the Martians', 'the Martians is', 'Martians is hardly', 'is hardly a', 'hardly a surprise', 'a surprise ,', 'surprise , despite', ', despite the', 'despite the destruction', 'the destruction by', 'destruction by cannon', 'by cannon of', 'cannon of some', 'of some of', 'some of their', 'of their machines', 'their machines .', 'machines . The', '. The Narrator', 'The Narrator ,', 'Narrator , traveling', ', traveling about', 'traveling about to', 'about to seek', 'to seek escape', 'seek escape ,', 'escape , sees', ', sees much', 'sees much of', 'much of what', 'of what Wells', 'what Wells terms', 'Wells terms \"', 'terms \" the', '\" the rout', 'the rout of', 'rout of Mankind', 'of Mankind \"', 'Mankind \" .', '\" . He', '. He encounters', 'He encounters a', 'encounters a curate', 'a curate endowed', 'curate endowed with', 'endowed with the', 'with the Victorian', 'the Victorian affliction', 'Victorian affliction of', 'affliction of a', 'of a much', 'a much too', 'much too precious', 'too precious and', 'precious and nervous', 'and nervous personality', 'nervous personality .', 'personality . They', '. They eventually', 'They eventually find', 'eventually find themselves', 'find themselves on', 'themselves on the', 'on the very', 'the very edge', 'very edge of', 'edge of a', 'of a Martian', 'a Martian nest', 'Martian nest ,', 'nest , where', ', where they', 'where they discover', 'they discover an', 'discover an awful', 'an awful fact', 'awful fact :', 'fact : the', ': the Martians', 'the Martians are', 'Martians are shown', 'are shown to', 'shown to be', 'to be vampires', 'be vampires who', 'vampires who consume', 'who consume their', 'consume their prey', 'their prey alive', 'prey alive in', 'alive in a', 'in a very', 'a very effective', 'very effective scene', 'effective scene .', 'scene . Wells', '. Wells adds', 'Wells adds that', 'adds that after', 'that after eating', 'after eating they', 'eating they set', 'they set up', 'set up \"', 'up \" a', '\" a prolonged', 'a prolonged and', 'prolonged and cheerful', 'and cheerful hooting', 'cheerful hooting \"', 'hooting \" .', '\" . The', '. The Narrator', 'The Narrator finally', 'Narrator finally is', 'finally is obliged', 'is obliged to', 'obliged to beat', 'to beat senseless', 'beat senseless the', 'senseless the increasingly', 'the increasingly hysterical', 'increasingly hysterical curate', 'hysterical curate \\x96', 'curate \\x96 who', '\\x96 who revives', 'who revives just', 'revives just as', 'just as the', 'as the Martians', 'the Martians drag', 'Martians drag him', 'drag him off', 'him off to', 'off to the', 'to the larder', 'the larder (', 'larder ( cheers', '( cheers from', 'cheers from the', 'from the gallery', 'the gallery ;', 'gallery ; British', '; British curates', 'British curates are', 'curates are so', 'are so often', 'so often utterly', 'often utterly insufferable).<br', 'utterly insufferable).<br /><br', 'insufferable).<br /><br />This', '/><br />This film', '/>This film lasts', 'film lasts almost', 'lasts almost 3', 'almost 3 hours', '3 hours ,', 'hours , going', ', going through', 'going through Wells', \"through Wells '\", \"Wells ' story\", \"' story in\", 'story in welcome', 'in welcome detail', 'welcome detail .', 'detail . It', \". It 's\", \"It 's about\", \"'s about time\", 'about time the', 'time the author', 'the author got', 'author got his', 'got his due', 'his due \\x96', 'due \\x96 in', '\\x96 in a', 'in a compelling', 'a compelling presentation', 'compelling presentation that', 'presentation that builds', 'that builds in', 'builds in dramatic', 'in dramatic impact', 'dramatic impact .', 'impact . A', '. A word', 'A word about', 'word about the', 'about the acting', 'the acting :', 'acting : Do', \": Do n't\", \"Do n't expect\", \"n't expect award\", 'expect award -', 'award - winning', '- winning performances', 'winning performances .', 'performances . They', \". They 're\", \"They 're not\", \"'re not bad\", 'not bad ,', 'bad , however', ', however ,', 'however , the', ', the actors', 'the actors are', 'actors are earnest', 'are earnest and', 'earnest and they', 'and they grow', 'they grow on', 'grow on you', 'on you .', 'you . Most', '. Most of', 'Most of them', 'of them ,', 'them , however', ', however ,', 'however , have', ', have had', 'have had very', 'had very abbreviated', 'very abbreviated film', 'abbreviated film careers', 'film careers ,', 'careers , often', ', often only', 'often only in', 'only in this', 'in this film', 'this film .', 'film . The', '. The Narrator', 'The Narrator is', 'Narrator is played', 'is played by', 'played by hunky', 'by hunky Anthony', 'hunky Anthony Piana', 'Anthony Piana ,', 'Piana , in', ', in his', 'in his 2nd', 'his 2nd film', '2nd film .', 'film . The', '. The Curate', 'The Curate is', 'Curate is John', 'is John Kaufman', 'John Kaufman \\x96', 'Kaufman \\x96 also', '\\x96 also in', 'also in his', 'in his 2nd', 'his 2nd film', '2nd film as', 'film as an', 'as an actor', 'an actor but', 'actor but who', 'but who has', 'who has had', 'has had more', 'had more experience', 'more experience directing', 'experience directing .', 'directing . The', '. The Brother', 'The Brother (', 'Brother ( \"', '( \" Henderson', '\" Henderson \"', 'Henderson \" )', '\" ) is', ') is played', 'is played with', 'played with some', 'with some conviction', 'some conviction by', 'conviction by W.', 'by W. Bernard', 'W. Bernard Bauman', 'Bernard Bauman in', 'Bauman in his', 'in his first', 'his first film', 'first film .', 'film . The', '. The Artilleryman', 'The Artilleryman ,', 'Artilleryman , the', ', the only', 'the only other', 'only other sizable', 'other sizable part', 'sizable part ,', 'part , is', ', is played', 'is played by', 'played by James', 'by James Lathrop', 'James Lathrop in', 'Lathrop in his', 'in his first', 'his first film.<br', 'first film.<br /><br', 'film.<br /><br />This', '/><br />This is', '/>This is overall', 'is overall a', 'overall a splendid', 'a splendid film', 'splendid film ,', 'film , portraying', ', portraying for', 'portraying for the', 'for the first', 'the first time', 'first time the', 'time the War', 'the War of', 'War of the', 'of the Worlds', 'the Worlds as', 'Worlds as Wells', 'as Wells wrote', 'Wells wrote it', 'wrote it .', 'it . Despite', '. Despite its', 'Despite its slight', 'its slight defects', 'slight defects ,', 'defects , it', ', it is', 'it is far', 'is far and', 'far and away', 'and away better', 'away better than', 'better than any', 'than any of', 'any of its', 'of its hyped', 'its hyped -', 'hyped - up', '- up competitors', 'up competitors .', 'competitors . If', '. If you', 'If you want', 'you want to', 'want to see', 'to see H.', 'see H. G.', 'H. G. Wells', \"G. Wells '\", \"Wells ' War\", \"' War of\", 'War of the', 'of the Worlds', 'the Worlds \\x96', 'Worlds \\x96 and', '\\x96 and not', 'and not some', 'not some wholly', 'some wholly distorted', 'wholly distorted version', 'distorted version of', 'version of it', 'of it \\x96', 'it \\x96 see', '\\x96 see this', 'see this film', 'this film !'], 'label': 'pos'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3201317"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J8IWeKMqYJA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1236b8d7-4976-4323-d026-cc301758d5f6"
      },
      "source": [
        "dictionary['the three leads'] # just an example"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCTNoiAmN-ND"
      },
      "source": [
        "mrkv = pd.DataFrame.from_dict(dictionary,orient=\"index\") # convert dictionary into pandas dataframe\n",
        "mrkv = mrkv.reset_index() # remove index\n",
        "\n",
        "mrkv.columns = ['trigrams','count'] # rename columns\n",
        "mrkv['bigrams'] =mrkv.trigrams.apply(lambda x: \" \".join(x.split(' ')[:2])) # create bigrams\n",
        "mrkv['output'] =mrkv.trigrams.apply(lambda x: x.split(' ')[2])\n",
        "inp_cnt = pd.DataFrame(mrkv.groupby(\"bigrams\",as_index=False)[\"count\"].sum()) # count frequency of bigrams model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JkMUXjDWt2q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "97ee018d-e642-40ec-b734-e248b55e8f78"
      },
      "source": [
        "inp_cnt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bigrams</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\b\b\b\bA Turkish</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\t \"</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\t Alex</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\t As</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\\t At</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1218998</th>\n",
              "      <td>… &lt;</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1218999</th>\n",
              "      <td>… although</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1219000</th>\n",
              "      <td>… but</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1219001</th>\n",
              "      <td>₤100 per</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1219002</th>\n",
              "      <td> \"</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1219003 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               bigrams  count\n",
              "0        \b\b\b\bA Turkish      1\n",
              "1                 \\t \"      2\n",
              "2              \\t Alex      1\n",
              "3                \\t As      3\n",
              "4                \\t At      1\n",
              "...                ...    ...\n",
              "1218998            … <      1\n",
              "1218999     … although      1\n",
              "1219000          … but      2\n",
              "1219001       ₤100 per      1\n",
              "1219002             \"      2\n",
              "\n",
              "[1219003 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHjoL-ZXrOWV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "821af2fb-84f1-47e4-839f-1d54a6b60e12"
      },
      "source": [
        "mrkv.head() # final dataframe for markov model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>trigrams</th>\n",
              "      <th>count</th>\n",
              "      <th>bigrams</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OK , I</td>\n",
              "      <td>31</td>\n",
              "      <td>OK ,</td>\n",
              "      <td>I</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>, I did</td>\n",
              "      <td>208</td>\n",
              "      <td>, I</td>\n",
              "      <td>did</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I did n't</td>\n",
              "      <td>883</td>\n",
              "      <td>I did</td>\n",
              "      <td>n't</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>did n't know</td>\n",
              "      <td>179</td>\n",
              "      <td>did n't</td>\n",
              "      <td>know</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>n't know what</td>\n",
              "      <td>211</td>\n",
              "      <td>n't know</td>\n",
              "      <td>what</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        trigrams  count   bigrams output\n",
              "0         OK , I     31      OK ,      I\n",
              "1        , I did    208       , I    did\n",
              "2      I did n't    883     I did    n't\n",
              "3   did n't know    179   did n't   know\n",
              "4  n't know what    211  n't know   what"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVAueDE2OpDh"
      },
      "source": [
        "**PART B: Change the output appropriately in ‘Simple Sentiment Analysis.ipynb’ to build an LSTM based language model. Plot the training performance as a function of epochs/iterations.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQYCYF09OqU-"
      },
      "source": [
        "SEED = 7\n",
        "# use seed for reproducibility\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(tokenize='spacy') # tokenize sentences\n",
        "LABEL = data.LabelField(dtype=torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ0WbdlTjx-F"
      },
      "source": [
        "from torchtext import datasets\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL) # import dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x39MyblMdah",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0885afea-6364-4655-f78b-d957dc7a0e86"
      },
      "source": [
        "# check data size of train and test data\n",
        "print(f'Number of training examples in data: {len(train_data)}')\n",
        "print(f'Number of testing examples in data: {len(test_data)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples in data: 25000\n",
            "Number of testing examples in data: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFTMcdcsMieC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "fb765b34-d4bb-4c35-8100-43a551cf5cc6"
      },
      "source": [
        "print(vars(train_data.examples[0])) # observe one example \n",
        "\n",
        "# split data into train and test data\n",
        "train_data, valid_data = train_data.split(random_state=random.seed(SEED),split_ratio=0.32) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['The', 'year', '2005', 'saw', 'no', 'fewer', 'than', '3', 'filmed', 'productions', 'of', 'H.', 'G.', 'Wells', \"'\", 'great', 'novel', ',', '\"', 'War', 'of', 'the', 'Worlds', '\"', '.', 'This', 'is', 'perhaps', 'the', 'least', 'well', '-', 'known', 'and', 'very', 'probably', 'the', 'best', 'of', 'them', '.', 'No', 'other', 'version', 'of', 'WotW', 'has', 'ever', 'attempted', 'not', 'only', 'to', 'present', 'the', 'story', 'very', 'much', 'as', 'Wells', 'wrote', 'it', ',', 'but', 'also', 'to', 'create', 'the', 'atmosphere', 'of', 'the', 'time', 'in', 'which', 'it', 'was', 'supposed', 'to', 'take', 'place', ':', 'the', 'last', 'year', 'of', 'the', '19th', 'Century', ',', '1900', '\\x85 ', 'using', 'Wells', \"'\", 'original', 'setting', ',', 'in', 'and', 'near', 'Woking', ',', 'England.<br', '/><br', '/>IMDb', 'seems', 'unfriendly', 'to', 'what', 'they', 'regard', 'as', '\"', 'spoilers', '\"', '.', 'That', 'might', 'apply', 'with', 'some', 'films', ',', 'where', 'the', 'ending', 'might', 'actually', 'be', 'a', 'surprise', ',', 'but', 'with', 'regard', 'to', 'one', 'of', 'the', 'most', 'famous', 'novels', 'in', 'the', 'world', ',', 'it', 'seems', 'positively', 'silly', '.', 'I', 'have', 'no', 'sympathy', 'for', 'people', 'who', 'have', 'neglected', 'to', 'read', 'one', 'of', 'the', 'seminal', 'works', 'in', 'English', 'literature', ',', 'so', 'let', \"'s\", 'get', 'right', 'to', 'the', 'chase', '.', 'The', 'aliens', 'are', 'destroyed', 'through', 'catching', 'an', 'Earth', 'disease', ',', 'against', 'which', 'they', 'have', 'no', 'immunity', '.', 'If', 'that', \"'s\", 'a', 'spoiler', ',', 'so', 'be', 'it', ';', 'after', 'a', 'book', 'and', '3', 'other', 'films', '(', 'including', 'the', '1953', 'classic', ')', ',', 'you', 'ought', 'to', 'know', 'how', 'this', 'ends.<br', '/><br', '/>This', 'film', ',', 'which', 'follows', 'Wells', \"'\", 'plot', 'in', 'the', 'main', ',', 'is', 'also', 'very', 'cleverly', 'presented', '\\x96', 'in', 'a', 'way', 'that', 'might', 'put', 'many', 'viewers', 'off', 'due', 'to', 'their', 'ignorance', 'of', 'late', '19th', '/', 'early', '20th', 'Century', 'photography', '.', 'Although', 'filmed', 'in', 'a', 'widescreen', 'aspect', ',', 'the', 'film', 'goes', 'to', 'some', 'lengths', 'to', 'give', 'an', 'impression', 'of', 'contemporaneity', '.', 'The', 'general', 'coloration', 'of', 'skin', 'and', 'clothes', 'display', 'a', 'sepia', 'tint', 'often', 'found', 'in', 'old', 'photographs', '(', 'rather', 'than', 'black', ')', '.', 'Colors', 'are', 'often', 'reminiscent', 'of', 'hand', '-', 'tinting', '.', 'At', 'other', 'times', ',', 'colors', 'are', 'washed', 'out', '.', 'These', 'variations', 'are', 'typical', 'of', 'early', 'films', ',', 'which', 'did', \"n't\", 'use', 'standardized', 'celluloid', 'stock', 'and', 'therefore', 'presented', 'a', 'good', 'many', 'changes', 'in', 'print', 'quality', ',', 'even', 'going', 'from', 'black', '/', 'white', 'to', 'sepia', '/', 'white', 'to', 'blue', '/', 'white', 'to', 'reddish', '/', 'white', 'and', 'so', 'on', '\\x96', 'as', 'you', \"'ll\", 'see', 'on', 'occasion', 'here', '.', 'The', 'special', 'effects', 'are', 'deliberately', 'retrograde', ',', 'of', 'a', 'sort', 'seen', 'even', 'as', 'late', 'as', 'the', '1920s', '\\x96', 'and', 'yet', 'the', 'Martians', 'and', 'their', 'machines', 'are', 'very', 'much', 'as', 'Wells', 'described', 'them', 'and', 'have', 'a', 'more', 'nearly', 'realistic', '\"', 'feel', '\"', '.', 'Some', 'of', 'effects', 'are', 'really', 'awkward', '\\x96', 'such', 'as', 'the', 'destruction', 'of', 'Big', 'Ben', '.', 'The', 'acting', 'is', 'often', 'more', 'in', 'the', 'style', 'of', 'that', 'period', 'than', 'ours', '.', 'Some', 'aspects', 'of', 'Victorian', 'dress', 'may', 'appear', 'odd', ',', 'particularly', 'the', 'use', 'of', 'pomade', 'or', 'brilliantine', 'on', 'head', 'and', 'facial', 'hair.<br', '/><br', '/>This', 'film', 'is', 'the', 'only', 'one', 'that', 'follows', 'with', 'some', 'closeness', 'Wells', \"'\", 'original', 'narrative', '\\x96', 'as', 'has', 'been', 'noted', '.', 'Viewers', 'may', 'find', 'it', 'informative', 'to', 'note', 'plot', 'details', 'that', 'appear', 'here', 'that', 'are', 'occasionally', 'retained', 'in', 'other', 'versions', 'of', 'the', 'story', '.', 'Wells', \"'\", 'description', 'of', 'the', 'Martians', '\\x96', 'a', 'giant', 'head', 'mounted', 'on', 'numerous', 'tentacles', '\\x96', 'is', 'effectively', 'portrayed', '.', 'When', 'the', 'Martian', 'machines', 'appear', ',', 'about', 'an', 'hour', 'into', 'the', 'film', ',', 'they', 'too', 'give', 'a', 'good', 'impression', 'of', 'how', 'Wells', 'described', 'them', '.', 'Both', 'Wells', 'and', 'this', 'film', 'do', 'an', 'excellent', 'job', 'of', 'portraying', 'the', 'progress', 'of', 'the', 'Martians', 'from', 'the', 'limited', 'perspective', '(', 'primarily', ')', 'of', 'rural', 'England', '\\x96', 'plus', 'a', 'few', 'scenes', 'in', 'London', '(', 'involving', 'the', 'Narrator', \"'s\", 'brother', ')', '.', 'The', 'director', 'is', 'unable', 'to', 'resist', 'showing', 'the', 'destruction', 'of', 'a', 'major', 'landmark', '(', 'Big', 'Ben', ')', ',', 'but', 'at', 'least', 'does', \"n't\", 'dwell', 'unduly', 'on', 'the', 'devastation', 'of', 'London.<br', '/><br', '/>The', 'victory', 'of', 'the', 'Martians', 'is', 'hardly', 'a', 'surprise', ',', 'despite', 'the', 'destruction', 'by', 'cannon', 'of', 'some', 'of', 'their', 'machines', '.', 'The', 'Narrator', ',', 'traveling', 'about', 'to', 'seek', 'escape', ',', 'sees', 'much', 'of', 'what', 'Wells', 'terms', '\"', 'the', 'rout', 'of', 'Mankind', '\"', '.', 'He', 'encounters', 'a', 'curate', 'endowed', 'with', 'the', 'Victorian', 'affliction', 'of', 'a', 'much', 'too', 'precious', 'and', 'nervous', 'personality', '.', 'They', 'eventually', 'find', 'themselves', 'on', 'the', 'very', 'edge', 'of', 'a', 'Martian', 'nest', ',', 'where', 'they', 'discover', 'an', 'awful', 'fact', ':', 'the', 'Martians', 'are', 'shown', 'to', 'be', 'vampires', 'who', 'consume', 'their', 'prey', 'alive', 'in', 'a', 'very', 'effective', 'scene', '.', 'Wells', 'adds', 'that', 'after', 'eating', 'they', 'set', 'up', '\"', 'a', 'prolonged', 'and', 'cheerful', 'hooting', '\"', '.', 'The', 'Narrator', 'finally', 'is', 'obliged', 'to', 'beat', 'senseless', 'the', 'increasingly', 'hysterical', 'curate', '\\x96', 'who', 'revives', 'just', 'as', 'the', 'Martians', 'drag', 'him', 'off', 'to', 'the', 'larder', '(', 'cheers', 'from', 'the', 'gallery', ';', 'British', 'curates', 'are', 'so', 'often', 'utterly', 'insufferable).<br', '/><br', '/>This', 'film', 'lasts', 'almost', '3', 'hours', ',', 'going', 'through', 'Wells', \"'\", 'story', 'in', 'welcome', 'detail', '.', 'It', \"'s\", 'about', 'time', 'the', 'author', 'got', 'his', 'due', '\\x96', 'in', 'a', 'compelling', 'presentation', 'that', 'builds', 'in', 'dramatic', 'impact', '.', 'A', 'word', 'about', 'the', 'acting', ':', 'Do', \"n't\", 'expect', 'award', '-', 'winning', 'performances', '.', 'They', \"'re\", 'not', 'bad', ',', 'however', ',', 'the', 'actors', 'are', 'earnest', 'and', 'they', 'grow', 'on', 'you', '.', 'Most', 'of', 'them', ',', 'however', ',', 'have', 'had', 'very', 'abbreviated', 'film', 'careers', ',', 'often', 'only', 'in', 'this', 'film', '.', 'The', 'Narrator', 'is', 'played', 'by', 'hunky', 'Anthony', 'Piana', ',', 'in', 'his', '2nd', 'film', '.', 'The', 'Curate', 'is', 'John', 'Kaufman', '\\x96', 'also', 'in', 'his', '2nd', 'film', 'as', 'an', 'actor', 'but', 'who', 'has', 'had', 'more', 'experience', 'directing', '.', 'The', 'Brother', '(', '\"', 'Henderson', '\"', ')', 'is', 'played', 'with', 'some', 'conviction', 'by', 'W.', 'Bernard', 'Bauman', 'in', 'his', 'first', 'film', '.', 'The', 'Artilleryman', ',', 'the', 'only', 'other', 'sizable', 'part', ',', 'is', 'played', 'by', 'James', 'Lathrop', 'in', 'his', 'first', 'film.<br', '/><br', '/>This', 'is', 'overall', 'a', 'splendid', 'film', ',', 'portraying', 'for', 'the', 'first', 'time', 'the', 'War', 'of', 'the', 'Worlds', 'as', 'Wells', 'wrote', 'it', '.', 'Despite', 'its', 'slight', 'defects', ',', 'it', 'is', 'far', 'and', 'away', 'better', 'than', 'any', 'of', 'its', 'hyped', '-', 'up', 'competitors', '.', 'If', 'you', 'want', 'to', 'see', 'H.', 'G.', 'Wells', \"'\", 'War', 'of', 'the', 'Worlds', '\\x96', 'and', 'not', 'some', 'wholly', 'distorted', 'version', 'of', 'it', '\\x96', 'see', 'this', 'film', '!'], 'label': 'pos'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFAXOmbDMsBf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "327fc6a2-5a80-431d-9544-e29698052506"
      },
      "source": [
        "print(f'Number of training examples: {len(train_data)}') # 8000\n",
        "print(f'Number of validation examples: {len(valid_data)}') # 17000\n",
        "print(f'Number of testing examples: {len(test_data)}') # 25000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 8000\n",
            "Number of validation examples: 17000\n",
            "Number of testing examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgmJo2owMuSK"
      },
      "source": [
        "TEXT.build_vocab(train_data, max_size=1500) # build vocabulary\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gqX3NwEMwUZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d855be11-44f4-4c93-9781-b9fa01367ee8"
      },
      "source": [
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 1502\n",
            "Unique tokens in LABEL vocabulary: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLVU5iRYMyCZ"
      },
      "source": [
        "BATCH_SIZE = 16 # specify batch size\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train_data, valid_data, test_data), batch_size=BATCH_SIZE, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEA2-n0PM0Jj"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim,hidden_dim,BATCH_SIZE,output_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, output_dim)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, x):        \n",
        "        embedded = self.embedding(x)\n",
        "        output, (hidden,cell) = self.rnn(embedded)\n",
        "        dim = output.size()\n",
        "        output = output.view(-1, output.shape[2])\n",
        "        output1 = F.log_softmax(output,dim=1)\n",
        "        \n",
        "        if BATCH_SIZE==dim[1]:\n",
        "            output1 = output1.view(-1,OUTPUT_DIM,BATCH_SIZE)\n",
        "        else:\n",
        "            output1 = output1.view(dim[1],OUTPUT_DIM,-1)\n",
        "        #output = [sent len, batch size, hid dim]\n",
        "        #hidden = [1, batch size, hid dim]       \n",
        "        return output1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvaG7UmsM2Di"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = len(TEXT.vocab)\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM,HIDDEN_DIM,BATCH_SIZE,OUTPUT_DIM) # built a RNN model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApgMbv-BM33n"
      },
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=1e-2) # SGD optimizer\n",
        "criterion = nn.NLLLoss()\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlkRxF4MM5ni"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion,BATCH_SIZE):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_label_count = 0 \n",
        "    loss=0\n",
        "    model.train()\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()   \n",
        "        predictions = model(batch.text)\n",
        "\n",
        "        dim = predictions.size()\n",
        "        if dim[2] !=BATCH_SIZE:\n",
        "            BATCH_SIZE = dim[2]\n",
        "            \n",
        "        pad = torch.tensor([1]*BATCH_SIZE,device=\"cuda:0\").view(BATCH_SIZE,-1)\n",
        "        _,preds = torch.max(predictions,1)\n",
        "        labels = batch.text.view(-1,BATCH_SIZE)\n",
        "        labels = labels[1:]\n",
        "        pad = torch.tensor([1]*BATCH_SIZE,device=\"cuda:0\").view(-1,BATCH_SIZE)\n",
        "        labels = torch.cat((labels,pad),0)\n",
        "        loss = criterion(predictions,labels)\n",
        "        acc = torch.sum(preds == labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        epoch_label_count+= labels.numel()\n",
        "        \n",
        "    return epoch_loss / len(iterator) , (epoch_acc /epoch_label_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jYkY460WIqQ"
      },
      "source": [
        "def evaluate(model, iterator, criterion, BATCH_SIZE):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_label_count = 0 \n",
        "    loss=0\n",
        "    model.train()\n",
        "    for batch in iterator: \n",
        "        predictions = model(batch.text)\n",
        "\n",
        "        dim = predictions.size()\n",
        "        if dim[2] !=BATCH_SIZE:\n",
        "            BATCH_SIZE = dim[2]\n",
        "            \n",
        "        pad = torch.tensor([1]*BATCH_SIZE,device=\"cuda:0\").view(BATCH_SIZE,-1)\n",
        "        _,preds = torch.max(predictions,1)\n",
        "        labels = batch.text.view(-1,BATCH_SIZE)\n",
        "        labels = labels[1:]\n",
        "        pad = torch.tensor([1]*BATCH_SIZE,device=\"cuda:0\").view(-1,BATCH_SIZE)\n",
        "        labels = torch.cat((labels,pad),0)\n",
        "        loss = criterion(predictions,labels)\n",
        "        acc = torch.sum(preds == labels)\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        epoch_label_count+= labels.numel()\n",
        "        \n",
        "    return epoch_loss / len(iterator) , (epoch_acc /epoch_label_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fiUwAHfWgKg"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7bzfpjMM7-m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "c10866bc-6793-4198-f14d-45f079a50df5"
      },
      "source": [
        "#Train loss and accuracy\n",
        "\n",
        "N_EPOCHS = 10 # specify the number of epochs\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, BATCH_SIZE)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, BATCH_SIZE)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "\n",
        "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')\n",
        "\n",
        "    print(f'Best validation loss: {best_valid_loss:.3f}% |')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 01 | Train Loss: 6.974 | Train Acc: 6.95% | Val. Loss: 7.306 | Val. Acc: 0.04% |\n",
            "Best validation loss: 7.306% |\n",
            "| Epoch: 02 | Train Loss: 6.786 | Train Acc: 7.19% | Val. Loss: 7.296 | Val. Acc: 0.04% |\n",
            "Best validation loss: 7.296% |\n",
            "| Epoch: 03 | Train Loss: 6.724 | Train Acc: 7.01% | Val. Loss: 7.282 | Val. Acc: 0.04% |\n",
            "Best validation loss: 7.282% |\n",
            "| Epoch: 04 | Train Loss: 6.642 | Train Acc: 7.15% | Val. Loss: 7.249 | Val. Acc: 0.04% |\n",
            "Best validation loss: 7.249% |\n",
            "| Epoch: 05 | Train Loss: 6.536 | Train Acc: 6.72% | Val. Loss: 7.177 | Val. Acc: 0.04% |\n",
            "Best validation loss: 7.177% |\n",
            "| Epoch: 06 | Train Loss: 6.414 | Train Acc: 6.55% | Val. Loss: 7.097 | Val. Acc: 0.04% |\n",
            "Best validation loss: 7.097% |\n",
            "| Epoch: 07 | Train Loss: 6.308 | Train Acc: 6.54% | Val. Loss: 7.037 | Val. Acc: 0.04% |\n",
            "Best validation loss: 7.037% |\n",
            "| Epoch: 08 | Train Loss: 6.206 | Train Acc: 6.63% | Val. Loss: 7.002 | Val. Acc: 0.04% |\n",
            "Best validation loss: 7.002% |\n",
            "| Epoch: 09 | Train Loss: 6.133 | Train Acc: 6.72% | Val. Loss: 6.984 | Val. Acc: 0.04% |\n",
            "Best validation loss: 6.984% |\n",
            "| Epoch: 10 | Train Loss: 6.105 | Train Acc: 6.57% | Val. Loss: 6.975 | Val. Acc: 0.04% |\n",
            "Best validation loss: 6.975% |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BqhU79MM_B0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0503aaa0-6779-4cd8-f1a8-5fcc65f52f4d"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set()\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(train_losses)\n",
        "plt.title(\"Training Performance by the number of Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5Z3yoxjWvEG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0841f6de-73fb-401a-8a8a-4d4a681944be"
      },
      "source": [
        "#Test loss and accuracy\n",
        "\n",
        "model.load_state_dict(torch.load('tut1-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion, BATCH_SIZE)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 6.977 | Test Acc: 0.04%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpWt3mnjjynt"
      },
      "source": [
        "## **QUESTION 2: For each model, describe the key design choices made. Briefly mention how each choice influences training time and generative quality.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0LsrUjyq4UT"
      },
      "source": [
        "\n",
        "Markov model we went with the trigrams and there was not computational complexity in computing the probabilities and prediction. For LSTM, there was an input layer followed by embedding then lstm layer. The input was the tokenized and output can also the tokenized text offsetted by 1 place. For instance, if input is [This,is,my,favorite,movie] then output is [is,my,favorite,movie, ...\n",
        "\n",
        "1.  We took 30% training data with max 1000 words with batch size 16 to train our lstm model.\n",
        "2. If we increased the number of words then the model couldn't be loaded successfullyas there was insufficient memory\n",
        "3. If we rose size of the training data to higher percentage such as 65% or 75%, the notebook got disconnected from the server (crashed). \n",
        "4. When we decreased the batch size to 1 or 2 then model took longer time to train and when we increased it to 64 or 128 then there was memory error in which we encountered (higher complexity). We settled at 'batch size' as 16.\n",
        "5. Due to this limited training size and words, the quality of reviews formed by prediction was affected considerably. The prediction didn't give out a plausible sentence as we would have liked.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXbHncGklHGX"
      },
      "source": [
        "## **QUESTION 3: For each model, starting with the phrase ”My favorite movie ”, sample the next few words and create a 20 word generated review.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfABMOOIUlBE"
      },
      "source": [
        "Prediction: Markov Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lY7NLHglJS1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "08be9b7f-b952-4d22-e8da-84ded72d43e5"
      },
      "source": [
        "def create_sentence(sentence,no_preds,predict_length):\n",
        "\n",
        "    predict_length -= len(sentence.split(\" \")) \n",
        "\n",
        "    for j in range(0,no_preds):\n",
        "        pred_sentence = sentence\n",
        "        bi = \" \".join(pred_sentence.split(\" \")[1:])\n",
        "\n",
        "        for i in range(0,predict_length):\n",
        "            poss_pred = np.array(mrkv.output[mrkv.bigrams==bi])\n",
        "            scores = np.array(mrkv[mrkv.bigrams==bi]['count'])\n",
        "            length = inp_cnt[inp_cnt.bigrams==bi]['count']\n",
        "            probs = scores/length.iloc[0]\n",
        "            pred = random.choice(list(enumerate(probs)))[0]\n",
        "            pred = poss_pred[pred]\n",
        "            pred_sentence = pred_sentence + \" \" + pred\n",
        "            bi = \" \".join([bi.split(\" \")[1],pred])\n",
        "\n",
        "        print(\"Generated Review \"+str(j+1)+\": \",pred_sentence+\"\\n\")\n",
        "create_sentence(\"My favorite movie\",5,20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated Review 1:  My favorite movie ( either Ben or Arthur , a magnificent job throughout , enjoying a night on Saturday (\n",
            "\n",
            "Generated Review 2:  My favorite movie of about 2 steps above Ed Wood.<br /><br />Not being a 30 second gag on a shooting\n",
            "\n",
            "Generated Review 3:  My favorite movie Dawn of the General , that incident is downright painful to watch.<br /><br />Ritter 's character ranks\n",
            "\n",
            "Generated Review 4:  My favorite movie a 45 minute Boy 's main methods of intrigue , neither very good female lead begins a\n",
            "\n",
            "Generated Review 5:  My favorite movie of Luchino Visconti is working way too standard . One pilot has a prop for the jokes\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqhoEGyUUqHE"
      },
      "source": [
        "Prediction: LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yeuhS1Qfrfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "08a39c13-673d-4ce6-b440-ffecd5d26e21"
      },
      "source": [
        "for j in range(5):\n",
        "    inp =torch.tensor([[TEXT.vocab.stoi[\"my\"]],[TEXT.vocab.stoi[\"favorite\"]],[TEXT.vocab.stoi[\"movie\"]]],device=\"cuda:0\")\n",
        "    for i in range(17):\n",
        "        op = model(inp)\n",
        "        op = op.squeeze(0)\n",
        "        op = op[:,op.size()[1]-1].detach().cpu().numpy()\n",
        "        op = np.sort(op)[::-1]\n",
        "        op = op[:500] \n",
        "        new_inp = torch.tensor(np.where(op==np.random.choice(op,1)),device=\"cuda:0\")\n",
        "        inp = torch.cat((inp,new_inp))\n",
        "    \n",
        "    generated_text = \"\"\n",
        "    for val in inp:\n",
        "        generated_text = generated_text + \" \" + TEXT.vocab.itos[val]\n",
        "    print(\"Generated Review \"+str(j+1)+\": \",generated_text+\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated Review 1:   my favorite movie nothing thought now sense least must name and black it acting n't about wo can As himself\n",
            "\n",
            "Generated Review 2:   my favorite movie cast awful seemed All day got remember come watch world never makes first everything /><br drama this\n",
            "\n",
            "Generated Review 3:   my favorite movie We To almost plays little down minutes them way He But hard make i people picture who\n",
            "\n",
            "Generated Review 4:   my favorite movie ( almost people old children always could money women who played & sort star through may father\n",
            "\n",
            "Generated Review 5:   my favorite movie well father felt played She end plot probably around pretty nothing You film together live since man\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}